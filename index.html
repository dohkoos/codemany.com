
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>乐者为王</title>
  <meta name="author" content="dohkoos">

  
  <meta name="description" content="事情的起因是想要注销某个免费的163邮箱，但是查找资料后发现： 163免费邮箱不支持直接注销。 如果连续180天没登录过网易任何产品的帐号，系统将自动清空所有信息和资料，并删除帐号。 那么问题来了，超过上述期限没有登录的话，如何才能验证该邮箱是否真的被销号？直接登录肯定不行， &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://codemany.com/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="乐者为王" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-46570161-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">乐者为王</a></h1>
  
    <h2>Do one thing, and do it well.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="codemany.com">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/how-to-verify-mailbox-exist/">如何验证邮箱是否存在？</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-12-23T11:45:10+08:00'><span class='date'><span class='date-month'>Dec</span> <span class='date-day'>23</span><span class='date-suffix'>rd</span>, <span class='date-year'>2017</span></span> <span class='time'>11:45 am</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>事情的起因是想要注销某个免费的163邮箱，但是查找资料后发现：</p>

<blockquote>
<p>163免费邮箱不支持直接注销。</p>

<p>如果连续180天没登录过网易任何产品的帐号，系统将自动清空所有信息和资料，并删除帐号。</p>
</blockquote>

<p>那么问题来了，超过上述期限没有登录的话，如何才能验证该邮箱是否真的被销号？直接登录肯定不行，如果那时邮箱正处于冻结状态，你登录不就相当于重新激活吗！或者向该邮箱发送测试邮件，如果投递成功就说明邮箱还没被注销，但这种方法太过麻烦，尤其对于注销多个邮箱的情况。要想优雅地解决这个问题，我们需要从了解电子邮件传输协议开始。</p>

<h3 id="电子邮件传输协议">电子邮件传输协议</h3>

<p>电子邮件传输协议主要包括SMTP协议、ESMTP协议、POP协议、IMAP协议等，具体的标准可以参考<a href="https://www.rfc-editor.org/">RFC Editor</a>网站下的相关RFC文档。POP3是Post Office Protocol 3的缩写，IMAP是Internet Message Access Protocol的缩写，两者的作用都是收取邮件，关键的区别在于邮件是放在服务器上（IMAP）还是放在本地电脑上（POP3）。SMTP是Simple Mail Transfer Protocol的缩写，ESMTP即Extension-SMTP，是为对付滥用SMTP服务器发邮件而引入的，它和SMTP的主要区别在于增加了发信认证机制。</p>

<p>下图是一个典型的电子邮件收发路径：</p>

<p><img src="/uploads/email-architecture.png" alt="email-architecture"></p>

<p>在你写完邮件点击“发送”时，你的邮件客户端会将消息发送给SMTP服务器（就如同我们将信交给本地的邮局），然后SMTP服务器根据收信人的地址向DNS服务器查询邮箱地址后缀的MX记录，找到目的邮件服务器（就好像邮局根据收信人的地址选择邮递路线，经过飞机或火车等交通工具到达收信人所在地邮局），然后收信人就可以使用自己的帐号登录POP3服务器收取邮件（就像收信人从邮箱取信）。</p>

<p>由此推断，通过SMTP服务器或者MX服务器能够验证邮箱是否存在。下面我们就来进行测试。</p>

<h3 id="实践">实践</h3>

<p>我们先从SMTP服务器开始。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>C:\&gt;telnet smtp.163.com 25
</span><span class='line'>220 163.com Anti-spam GT for Coremail System (163com[20141201])
</span><span class='line'>
</span><span class='line'>hello    # SMTP协议没有这个指令
</span><span class='line'>502 Error: command not implemented
</span><span class='line'>
</span><span class='line'>helo hi  # 因为每次按键都会被传送到服务器，所以输入错误时不能使用退格键删除，只能换行重新输入
</span><span class='line'>500 Error: bad syntax
</span><span class='line'>
</span><span class='line'>helo hi
</span><span class='line'>250 OK
</span><span class='line'>
</span><span class='line'>vrfy noname@163.com  # 现在的邮件服务器都使用ESTMP协议，VRFY、EXPN这些指令都已经被禁用或不被支持
</span><span class='line'>502 Error: command not implemented
</span><span class='line'>
</span><span class='line'>mail from: &lt;noname@example.com&gt;  # 必须是同域的邮箱才能发邮件
</span><span class='line'>553 Local user only,163 smtp7,C8CowAA3eUZ7jEBaXE8hDg--.38995S2 1514179735
</span><span class='line'>mail from: &lt;noname@163.com&gt;
</span><span class='line'>553 authentication is required,163 smtp7,C8CowAA3eUZ7jEBaXE8hDg--.38995S3 1514179751
</span><span class='line'>
</span><span class='line'>quit
</span><span class='line'>221 Bye</span></code></pre></td></tr></table></div></figure>

<p>因为邮件服务器使用ESMTP协议的缘故，必须使用同域的其它邮箱登录SMTP服务器后才能验证邮箱是否存在，这显然不是什么好的验证方法。</p>

<p>再来试试MX服务器，希望不要如此麻烦，否则的话就完蛋了～</p>

<p>首先找到163免费邮箱的MX服务器：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>C:\&gt;nslookup -q=mx 163.com
</span><span class='line'>Non-authoritative answer:
</span><span class='line'>163.com MX preference = 10, mail exchanger = 163mx03.mxmail.netease.com
</span><span class='line'>163.com MX preference = 10, mail exchanger = 163mx01.mxmail.netease.com
</span><span class='line'>163.com MX preference = 10, mail exchanger = 163mx02.mxmail.netease.com
</span><span class='line'>163.com MX preference = 50, mail exchanger = 163mx00.mxmail.netease.com
</span><span class='line'>
</span><span class='line'>163.com nameserver = ns6.nease.net
</span><span class='line'>163.com nameserver = ns3.nease.net
</span><span class='line'>163.com nameserver = ns4.nease.net
</span><span class='line'>163.com nameserver = ns1.nease.net
</span><span class='line'>163.com nameserver = ns8.166.com
</span><span class='line'>163.com nameserver = ns2.166.com
</span><span class='line'>163.com nameserver = ns5.nease.net
</span><span class='line'>163mx01.mxmail.netease.com      internet address = 220.181.14.138
</span><span class='line'>163mx01.mxmail.netease.com      internet address = 220.181.14.139
</span><span class='line'>163mx01.mxmail.netease.com      internet address = 220.181.14.140
</span><span class='line'>163mx01.mxmail.netease.com      internet address = 220.181.14.141
</span><span class='line'>163mx01.mxmail.netease.com      internet address = 220.181.14.142
</span><span class='line'>163mx01.mxmail.netease.com      internet address = 220.181.14.143
</span><span class='line'>163mx01.mxmail.netease.com      internet address = 220.181.14.135
</span><span class='line'>163mx01.mxmail.netease.com      internet address = 220.181.14.136
</span><span class='line'>163mx01.mxmail.netease.com      internet address = 220.181.14.137</span></code></pre></td></tr></table></div></figure>

<p>找到MX服务器后，我们就可以像对待SMTP服务器那样对待它：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>C:\&gt;telnet 163mx03.mxmail.netease.com 25
</span><span class='line'>220 163.com Anti-spam GT for Coremail System (163com[20141201])
</span><span class='line'>
</span><span class='line'>helo hi
</span><span class='line'>250 OK
</span><span class='line'>
</span><span class='line'>vrfy noname@163.com              # VRFY指令也处于禁用
</span><span class='line'>502 Error: command not implemented
</span><span class='line'>
</span><span class='line'>mail from: &lt;noname@example.com&gt;  # 注意：不同域的邮箱也能发邮件啦
</span><span class='line'>250 Mail OK
</span><span class='line'>
</span><span class='line'>rcpt to: &lt;fewfwe&gt;
</span><span class='line'>550 Invalid User: fewfwe
</span><span class='line'>
</span><span class='line'>rcpt to: &lt;fewfwe@163.com&gt;        # 邮箱不存在
</span><span class='line'>550 User not found: fewfwe@163.com
</span><span class='line'>
</span><span class='line'>rcpt to: &lt;fake@163.com&gt;          # 邮箱存在
</span><span class='line'>250 Mail OK
</span><span class='line'>
</span><span class='line'>quit
</span><span class='line'>221 Bye</span></code></pre></td></tr></table></div></figure>

<h3 id="总结">总结</h3>

<ul>
<li>验证邮箱是否存在的最好方法是在MX服务器上使用RCPT TO指令。</li>
<li>连续180天没登录过的163免费邮箱不会被注销。</li>
</ul>

<p>写到这里才想起我手里还有以前收集的大批邮箱，即使按照上面的步骤逐个验证也不现实，需要找个时间码个程序来自动化批量处理它们。这次就暂时先这样子吧。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/crawl-novel-with-scrapy-part6/">使用Scrapy爬取小说（6）</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-12-06T17:18:51+08:00'><span class='date'><span class='date-month'>Dec</span> <span class='date-day'>6</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>5:18 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>在<a href="/blog/crawl-novel-with-scrapy-part1/">使用Scrapy爬取小说（1）</a>中，我们首先抓取小说目录页面的所有章节链接，然后再使用这些链接分别抓取各个章节的正文内容。这样需要分析两个不同页面的结构。其实，还有个更简单点的方法，只要分析内容页面的结构即可。</p>

<p>在小说的内容页面中通常都会有“上一页”和“下一页”这样的链接。我们只需要先抓取小说的首个章节，然后抽取出它的“下一页”链接，接着抓取这个链接对应的章节，再抽取出这个章节的“下一页”链接，重复这个循环直到不再有“下一页”链接为止。</p>

<p>抽取“下一页”链接的XPath表达式是：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>//a[text()='下一页']/@href</span></code></pre></td></tr></table></div></figure>

<p>修改后的novel_spider.py如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>class NovelSpider(scrapy.Spider):
</span><span class='line'>  name = 'novelspider'
</span><span class='line'>  allowed_domains = ['example.com']
</span><span class='line'>  # 把原来的目录页链接改成第一章的链接
</span><span class='line'>  start_urls = ['http://example.com/wuxia/hzlz/ssjx/001.htm']
</span><span class='line'>
</span><span class='line'>  def parse(self, response):
</span><span class='line'>    # 有些小说网站的页眉和页脚都有“下一页”链接，我们
</span><span class='line'>    # 不需要管它有几个，只要获取第一个出现的就行。
</span><span class='line'>    link = response.xpath("//a[text()='下一页']/@href").extract_first()
</span><span class='line'>    next_url = response.urljoin(link)
</span><span class='line'>    yield scrapy.Request(next_url, callback=self.parse)  # 注意callback的值
</span><span class='line'>
</span><span class='line'>    item = NovelItem()
</span><span class='line'>    title = response.xpath('//center/table/tr[2]/td/text()').extract()
</span><span class='line'>    content = response.xpath('//center/table/tr[4]/td/text()').extract()
</span><span class='line'>    item['title'] = title
</span><span class='line'>    item['content'] = content
</span><span class='line'>    return item</span></code></pre></td></tr></table></div></figure>

<p>满怀愉悦地运行Spider，结果什么都没有，抓取失败。这是怎么回事？百思不得其解。</p>

<h3 id="郁闷之旅由此开始～">郁闷之旅由此开始～</h3>

<p>把yield关键词删掉试试？！修改后的代码如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>link = response.xpath("//a[text()='下一页']/@href").extract_first()
</span><span class='line'>next_url = response.urljoin(link)
</span><span class='line'>scrapy.Request(next_url, callback=self.parse)</span></code></pre></td></tr></table></div></figure>

<p>结果只输出第一章的内容。</p>

<p>把parse方法改成下面这样呢？</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>def parse(self, response):
</span><span class='line'>  item = NovelItem()
</span><span class='line'>  title = response.xpath('//center/table/tr[2]/td/text()').extract()
</span><span class='line'>  content = response.xpath('//center/table/tr[4]/td/text()').extract()
</span><span class='line'>  item['title'] = title
</span><span class='line'>  item['content'] = content
</span><span class='line'>  yield item
</span><span class='line'>
</span><span class='line'>  link = response.xpath("//a[text()='下一页']/@href").extract_first()
</span><span class='line'>  next_url = response.urljoin(link)
</span><span class='line'>  return scrapy.Request(next_url, callback=self.parse)</span></code></pre></td></tr></table></div></figure>

<p>也只能输出第一章的内容。</p>

<p>读到这里可能有看官会疑惑，为什么我会做出上面这两种修改？首先，我知道问题是在yield语句那里。其次，因为在我的印象中，yield的作用是延迟执行后面的语句。但现在从实际执行的情况来看显然不是如此。</p>

<h3 id="真实的yield">真实的yield</h3>

<p>查阅资料以后发现，包含yield的函数不再是普通函数，Python解释器会将其视为生成器。我们用个例子来详细说明下。</p>

<p>Fibonacci数列是个非常简单的递归数列，除第1和第2个数外，任何一个数都可以由前两个数相加得到。下面是输出Fibonacci数列前N个数的测试代码：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>def fab(max):
</span><span class='line'>  n = 0
</span><span class='line'>  a, b = 0, 1
</span><span class='line'>  while n &lt; max:
</span><span class='line'>    yield b
</span><span class='line'>    a, b = b, a + b
</span><span class='line'>    n = n + 1
</span><span class='line'>
</span><span class='line'>print(fab(5))</span></code></pre></td></tr></table></div></figure>

<p>猜猜会输出什么？有人以为会打印出Fibonacci数列，但实际上输出的是生成器对象的信息：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;generator object fab at 0x7fd41edfcd58&gt;</span></code></pre></td></tr></table></div></figure>

<p>fab(5)看起来像函数调用，但它其实是返回一个生成器对象。在对其调用next函数之前，它不会执行任何函数代码。虽然仍按函数的流程执行，但每执行到一条yield语句就会中断，并返回一个迭代值，下次执行时从yield处继续执行。看起来就好像一个函数在正常执行的过程中被yield中断了数次，每次中断都会通过yield返回当时的迭代值。</p>

<p>所以正确的打印代码应该这样写（for循环会自动对生成器对象调用next函数）：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>for n in fab(5)
</span><span class='line'>  print(n)</span></code></pre></td></tr></table></div></figure>

<p>也可以手动调用next函数，这样我们就可以更清楚地看到它的执行流程：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt;&gt;&gt; f = fab(5)
</span><span class='line'>&gt;&gt;&gt; next(f)
</span><span class='line'>1
</span><span class='line'>&gt;&gt;&gt; next(f)
</span><span class='line'>1
</span><span class='line'>&gt;&gt;&gt; next(f)
</span><span class='line'>2
</span><span class='line'>&gt;&gt;&gt; next(f)
</span><span class='line'>3
</span><span class='line'>&gt;&gt;&gt; next(f)
</span><span class='line'>5
</span><span class='line'>&gt;&gt;&gt; next(f)
</span><span class='line'>Traceback (most recent call last):
</span><span class='line'> File "&lt;stdin&gt;", line 1, in &lt;module&gt;
</span><span class='line'>StopIteration</span></code></pre></td></tr></table></div></figure>

<p>当函数执行结束时，生成器自动抛出StopIteration异常，表示迭代完成。在for循环中，无需处理StopIteration异常，循环会正常结束。</p>

<h3 id="正确的解决方案">正确的解决方案</h3>

<p>明白yield到底起什么作用后，再回过头来看前面的修改。</p>

<p>第2次修改明显是没有思考，胡乱盲目地尝试。它只是创建一个Request对象而已，能有什么用！代码也清楚地表明，parse方法仅会被回调1次，这也是仅能输出第一章内容的原因。现在回想起来
怎么也弄不明白当初为什么会这么写？！</p>

<p>第3次修改的代码首先是解析Response对象，把第一章的标题和内容封装起来返回给Scrapy。因为这个函数是生成器函数，所以Scrapy在输出第一章内容后会继续执行yield item后面的语句，直到函数结束，抛出StopIteration异常。因为有return语句，所以会把返回的值当作StopIteration异常的属性。</p>

<p>第1次修改和第3次修改类似，不同的是它返回“下一页”的Request对象给Scrapy，所以parse方法会被不断地循环回调，直到没有“下一页”为止。</p>

<p>归根结底，第1和第3次修改不能输出所有章节内容的原因是因为StopIteration异常吞掉了return返回的值，所以也要把它改成yield。修改后的代码如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>def parse(self, response):
</span><span class='line'>  item = NovelItem()
</span><span class='line'>  title = response.xpath('//center/table/tr[2]/td/text()').extract()
</span><span class='line'>  content = response.xpath('//center/table/tr[4]/td/text()').extract()
</span><span class='line'>  item['title'] = title
</span><span class='line'>  item['content'] = content
</span><span class='line'>  yield item
</span><span class='line'>
</span><span class='line'>  link = response.xpath("//a[text()='下一页']/@href").extract_first()
</span><span class='line'>  next_url = response.urljoin(link)
</span><span class='line'>  yield scrapy.Request(next_url, callback=self.parse)</span></code></pre></td></tr></table></div></figure>

<p>给代码做个美容：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>def parse(self, response):
</span><span class='line'>  yield {
</span><span class='line'>    'title': response.xpath('//center/table/tr[2]/td/text()').extract(),
</span><span class='line'>    'content': response.xpath('//center/table/tr[4]/td/text()').extract(),
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  link = response.xpath("//a[text()='下一页']/@href").extract_first()
</span><span class='line'>  next_url = response.urljoin(link)
</span><span class='line'>  yield scrapy.Request(next_url, callback=self.parse)</span></code></pre></td></tr></table></div></figure>

<p>打完收工。写文章比写代码累多了！</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/crawl-novel-with-scrapy-part5/">使用Scrapy爬取小说（5）</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-11-18T21:55:30+08:00'><span class='date'><span class='date-month'>Nov</span> <span class='date-day'>18</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>9:55 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>今天的任务是重构TxtPipeline。</p>

<p>先看下TxtPipeline中负责写文件的代码片段：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>f = open(filename, 'w')
</span><span class='line'>f.write(title)
</span><span class='line'>f.write(content)
</span><span class='line'>f.close()</span></code></pre></td></tr></table></div></figure>

<p>我们经常会看到这样的代码，但它存在严重的问题，你能把它找出来吗？</p>

<p>首先，open函数打开文件，并返回文件句柄，该句柄是由操作系统分配的。接着就是调用write方法写文件。最后是调用close方法关闭文件。文件使用完毕后必须关闭，因为文件句柄会占用操作系统的资源，并且操作系统同一时间能打开的文件数量也是有限的。</p>

<p>由于文件读写时都有可能产生IOError，一旦出错，后面的close方法就不会被调用。所以，为确保无论是否出错都能正常地关闭文件和释放文件句柄，我们需要使用如下方法来实现：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>try:
</span><span class='line'>  f = open(filename, 'w')
</span><span class='line'>  f.write(title)
</span><span class='line'>  f.write(content)
</span><span class='line'>finally:
</span><span class='line'>  if f:
</span><span class='line'>    f.close()</span></code></pre></td></tr></table></div></figure>

<p>但每次都这么写实在太繁琐。其实，我们可以使用with语句来帮我们自动管理文件资源：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>with open(filename, 'w') as f:
</span><span class='line'>  f.write(title)
</span><span class='line'>  f.write(content)</span></code></pre></td></tr></table></div></figure>

<p>这和前面的try-finally是一样的，但是代码更加简洁，而且不必调用close方法。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/crawl-novel-with-scrapy-part4/">使用Scrapy爬取小说（4）</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-11-11T14:17:36+08:00'><span class='date'><span class='date-month'>Nov</span> <span class='date-day'>11</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>2:17 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>在<a href="/blog/crawl-novel-with-scrapy-part1/">使用Scrapy爬取小说（1）</a>中，我们使用range(1, 310)来确定章节链接的范围，这很不好。我们编程的时候应该要尽量注意减少代码中的硬编码和魔数，提高代码的可移植性。如果小说章节的链接不是这种连续的数字，或者章节的数量是在逐步增加的，那么这段代码就是无效的，或者会慢慢变得无效。</p>

<p>如何才能把这段代码写得更具可移植性呢？</p>

<p><img src="/uploads/novel-chapter-urls.png" alt="novel-chapter-urls"></p>

<p>我们不必关心章节链接的格式，我们只要知道它是个链接，一定是以<code>&lt;a href=&quot;url&quot;&gt;text&lt;/a&gt;</code>这种形式呈现（如上图所示）。我们也不必关心章节数量是否变化，只要把所有这种形式的链接抓取下来即可。与此对应的XPath表达式是：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>//center/table[@bordercolorlight]//a/@href</span></code></pre></td></tr></table></div></figure>

<p>因为页面文档中可能有多个表格，所以要在table后面添加@bordercolorlight属性来指定我们要查找的那个。</p>

<p>那么在Scrapy中如何实现这样的能力呢？以下是具体的代码：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>def parse(self, response):
</span><span class='line'>  links = response.xpath('//center/table[@bordercolorlight]//a/@href').extract()
</span><span class='line'>  for link in links:
</span><span class='line'>    next = response.urljoin(link)
</span><span class='line'>    yield scrapy.Request(next, callback=self.parse_chapter)</span></code></pre></td></tr></table></div></figure>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/crawl-novel-with-scrapy-part3/">使用Scrapy爬取小说（3）</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-11-04T09:55:53+08:00'><span class='date'><span class='date-month'>Nov</span> <span class='date-day'>4</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>9:55 am</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>在前文中，我们将小说的每个章节保存为独立的文本文件。今天我们准备把小说内容输出到数据库。对于数据存储，我选择MongoDB。为什么是MongoDB而不是其它？原因是以前没用过，想尝试下。</p>

<p>现在我们已经知道，要把抓取来的数据保存到数据库，只需实现Item Pipeline即可。我们可以仿照前面的实现依葫芦画瓢。</p>

<p>以下是将小说内容保存到MongoDB的代码：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import pymongo
</span><span class='line'>
</span><span class='line'>class MongoPipeline(object):
</span><span class='line'>  def open_spider(self, spider):
</span><span class='line'>    self.client = pymongo.MongoClient('localhost', 27017)
</span><span class='line'>    self.novel = self.client['novel']
</span><span class='line'>    self.ssjx = self.novel['ssjx']
</span><span class='line'>
</span><span class='line'>  def process_item(self, item, spider):
</span><span class='line'>    data = {
</span><span class='line'>      # 标题和内容都是列表类型，必须先转换成字符串
</span><span class='line'>      'title' : ''.join(item['title']),
</span><span class='line'>      'content' : ''.join(item['content']),
</span><span class='line'>    }
</span><span class='line'>    self.ssjx.insert(data)
</span><span class='line'>    return item
</span><span class='line'>
</span><span class='line'>  def close_spider(self, spider):
</span><span class='line'>    self.client.close()</span></code></pre></td></tr></table></div></figure>

<p>将组件添加到novel/settings.py的ITEM_PIPELINES配置中以启用它：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ITEM_PIPELINES = {
</span><span class='line'>  'novel.pipelines.TxtPipeline' : 300,
</span><span class='line'>  'novel.pipelines.MongoPipeline' : 400,
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

<p>在项目的根目录中使用下面的指令运行Spider：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scrapy crawl novelspider</span></code></pre></td></tr></table></div></figure>

<p>如果没有问题的话，爬虫会不停地运行，小说的章节内容也会被一个个地保存到数据库。下面的截图是最终的抓取结果：</p>

<p><img src="/uploads/novel-mongo-gui.png" alt="novel-mongo-gui"></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/crawl-novel-with-scrapy-part2/">使用Scrapy爬取小说（2）</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-10-28T20:35:23+08:00'><span class='date'><span class='date-month'>Oct</span> <span class='date-day'>28</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>8:35 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>接上文。</p>

<p>将抓取的小说内容保存成本地文件是通过在命令行指定<code>-o</code>选项实现的。虽然工作的很好，但是有两个缺点：一是把所有小说内容保存到单个文件会导致该文件太大，用文本编辑器打开随机浏览的速度非常慢；二是小说章节不是按照顺序保存的，导致阅读指定的章节内容很不方便。</p>

<p>再写个小工具按章节内容分割小说文件？无需如此麻烦。我们可以在Scrapy中直接将每个章节保存为单独的文本文件。Scrapy中的Item Pipeline就是干这类事情的。看下面的Scrapy架构图：</p>

<p><img src="/uploads/scrapy-architecture.png" alt="scrapy-architecture"></p>

<p>当Item在Spider中被收集之后，它们会被传递到Item Pipeline，这些Pipeline组件按照一定的顺序执行对Item的处理，同时也决定此Item是否继续通过，或是被丢弃而不再进行处理。</p>

<p>以下是Item Pipeline的一些典型应用：</p>

<ul>
<li>清理HTML数据</li>
<li>验证爬取的数据</li>
<li>查重</li>
<li>将爬取结果保存到数据库中</li>
</ul>

<h3 id="编写item-pipeline">编写Item Pipeline</h3>

<p>编写自己的Item Pipeline非常简单，每个Item Pipeline都是实现以下方法的Python类：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>process_item(self, item, spider)</span></code></pre></td></tr></table></div></figure>

<p>此外，下面的方法是可选实现的：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>open_spider(self, spider)  # 该方法在Spider被开启时调用
</span><span class='line'>close_spider(spider)       # 该方法在Spider被关闭时调用</span></code></pre></td></tr></table></div></figure>

<p>明白原理后，我们就可以开始编写自己的Item Pipeline。以下就是将小说的每个章节写成单独文本文件的代码：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>class TxtPipeline(object):
</span><span class='line'>  def process_item(self, item, spider):
</span><span class='line'>    # 标题和内容都是列表类型，必须先转换成字符串
</span><span class='line'>    title = ''.join(item['title'])
</span><span class='line'>    content = ''.join(item['content'])
</span><span class='line'>    # 使用章节名来创建文件
</span><span class='line'>    # 使用strip()来过滤非法字符r'\/:*?"&lt;&gt;|'
</span><span class='line'>    filename = '{}.txt'.format(title.strip())
</span><span class='line'>    f = open(filename, 'w')
</span><span class='line'>    f.write(title)
</span><span class='line'>    f.write(content)
</span><span class='line'>    f.close()
</span><span class='line'>    return item</span></code></pre></td></tr></table></div></figure>

<h3 id="启用item-pipeline">启用Item Pipeline</h3>

<p>要启用Pipeline组件，你必须将它添加到novel/settings.py的ITEM_PIPELINES配置中，就像下面这样:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ITEM_PIPELINES = {
</span><span class='line'>  'novel.pipelines.TxtPipeline' : 300,
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

<p>Pipeline后面的整数值确定它们的运行顺序，Item按数字从低到高通过每个Pipeline。通常将这些值定义在0-1000范围内。</p>

<h3 id="运行spider">运行Spider</h3>

<p>在项目的根目录中执行如下的命令（因为不再把所有的小说内容保存为单个文件，所有不需要指定<code>-o</code>选项）：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scrapy crawl novelspider</span></code></pre></td></tr></table></div></figure>

<p>没有报错的话，等个几分钟，就能看到很多文本文件躺在自己的电脑上面。</p>

<p><img src="/uploads/novel-txt-list.png" alt="novel-txt-list"></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/crawl-novel-with-scrapy-part1/">使用Scrapy爬取小说（1）</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-10-21T08:49:54+08:00'><span class='date'><span class='date-month'>Oct</span> <span class='date-day'>21</span><span class='date-suffix'>st</span>, <span class='date-year'>2017</span></span> <span class='time'>8:49 am</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>这几天正在看《Python网络数据采集》，在这过程中觉得有必要写个爬虫来实践学到的知识。便给自己定个小目标：试着用Scrapy爬取小说《蜀山剑侠传》，并把内容保存到本地文件中。</p>

<p>Scrapy是一个开源的Python数据抓取框架，速度快且强大，而且使用简单，可以很方便地抓取网站页面并从中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。</p>

<p>好吧，废话不多说，让我们直接开干！</p>

<h3 id="创建项目">创建项目</h3>

<p>在抓取之前，必须先创建一个Scrapy项目，可以直接用以下命令生成：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scrapy startproject novel</span></code></pre></td></tr></table></div></figure>

<p>这是新建项目的目录结构：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>.
</span><span class='line'>├── novel                # 项目模块
</span><span class='line'>│   ├── __init__.py
</span><span class='line'>│   ├── items.py         # 定义爬取的数据
</span><span class='line'>│   ├── middlewares.py   # 定义爬取时的中间件
</span><span class='line'>│   ├── pipelines.py     # 定义数据管道
</span><span class='line'>│   ├── __pycache__
</span><span class='line'>│   ├── settings.py      # 项目的设置文件
</span><span class='line'>│   └── spiders          # 放置爬虫代码的文件夹
</span><span class='line'>│       ├── __init__.py
</span><span class='line'>│       └── __pycache__
</span><span class='line'>└── scrapy.cfg           # Scrapy部署时的配置文件</span></code></pre></td></tr></table></div></figure>

<h3 id="分析页面结构">分析页面结构</h3>

<p>主要分析两个页面。一是小说的目录页面，目的是获取小说所有章节的链接以备抓取。二是任意章节页面，用于爬取其中的标题和正文。</p>

<p>通过观察目录页面的源码可以发现，所有章节的链接都类似<code>NUMBER.htm</code>。其中，NUMBER是3位整数，从001到309。</p>

<p><img src="/uploads/novel-chapter-urls.png" alt="novel-chapter-urls"></p>

<p>使用浏览器的检查器（Inspector）查看章节页面，尝试把光标放在正文上，你应该可以看到正文周围的蓝色方块（如下图左侧所示），如果你点击这个方块，就可以选中检查器中相应的HTML代码。可以看到小说的标题和正文都在td标签中。</p>

<p><img src="/uploads/novel-page-inspector.png" alt="novel-page-inspector"></p>

<p>与此对应的XPath表达式分别是：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>//center/table/tr[2]/td/text()  # 标题的XPath路径
</span><span class='line'>//center/table/tr[4]/td/text()  # 正文的XPath路径</span></code></pre></td></tr></table></div></figure>

<p>需要注意的是，上面XPath表达式里的中括号内的数字为节点索引，是从1开始的，而不是0。</p>

<h3 id="定义爬取的数据">定义爬取的数据</h3>

<p>当需要从某个网站抓取信息时，首先是定义我们要爬取的数据。在Scrapy中，可以通过Item来完成。以下是我们定义的Item：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import scrapy
</span><span class='line'>
</span><span class='line'>class NovelItem(scrapy.Item):
</span><span class='line'>  title = scrapy.Field()
</span><span class='line'>  content = scrapy.Field()</span></code></pre></td></tr></table></div></figure>

<h3 id="编写爬取数据的spider">编写爬取数据的Spider</h3>

<p>现在我们需要添加一个爬虫来真正做点什么。创建文件novel/spiders/novel_spider.py，添加如下内容：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import scrapy
</span><span class='line'>from novel.items import NovelItem
</span><span class='line'>
</span><span class='line'>class NovelSpider(scrapy.Spider):
</span><span class='line'>  name = 'novelspider'
</span><span class='line'>  allowed_domains = ['example.com']
</span><span class='line'>  start_urls = ['http://example.com/wuxia/hzlz/ssjx/']
</span><span class='line'>
</span><span class='line'>  def parse(self, response):
</span><span class='line'>    # 还记得前面分析目录页面时的结果吗：000、001...309。
</span><span class='line'>    for i in range(1, 310):
</span><span class='line'>      # 生成每个章节的绝对链接
</span><span class='line'>      next = response.urljoin('{0:03d}.htm'.format(i))
</span><span class='line'>      # 生成新的请求对象解析小说的标题和正文
</span><span class='line'>      yield scrapy.Request(next, callback=self.parse_chapter)
</span><span class='line'>
</span><span class='line'>  def parse_chapter(self, response):
</span><span class='line'>    item = NovelItem()
</span><span class='line'>    title = response.xpath('//center/table/tr[2]/td/text()').extract()
</span><span class='line'>    print('Title is', title)
</span><span class='line'>    content = response.xpath('//center/table/tr[4]/td/text()').extract()
</span><span class='line'>    print('Content is', content)
</span><span class='line'>    item['title'] = title
</span><span class='line'>    item['content'] = content
</span><span class='line'>    return item</span></code></pre></td></tr></table></div></figure>

<h3 id="运行spider">运行Spider</h3>

<p>完成爬虫后，如何通过它来得到我们想要的结果呢？在项目的根目录中执行如下的命令：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>scrapy crawl novelspider -o novel.json</span></code></pre></td></tr></table></div></figure>

<p>没有报错的话，等个几分钟，就能看到一个完整的JSON数据文件躺在自己的电脑上面。</p>

<p>不过如果打开的话，可能只会看到“\uXXXX”这样的乱码，它们都是中文字符的Unicode编码。要直接显示成中文的话，需要在novel/settings.py中添加以下设置：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>FEED_EXPORT_ENCODING = 'utf-8'</span></code></pre></td></tr></table></div></figure>

<p>最终的结果如图：</p>

<p><img src="/uploads/novel-json-chinese.png" alt="novel-json-chinese"></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/thinking-on-software-version/">软件版本号的思考</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-09-30T07:45:55+08:00'><span class='date'><span class='date-month'>Sep</span> <span class='date-day'>30</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>7:45 am</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><img src="/uploads/software-version.png" alt="software-version"></p>

<p>产品的名称用来表明产品目的，通常在产品的整个生命周期中都使用它。软件产品的版本号则用来表明产品在特定时间段内所拥有的功能集。它们关注的是以何种方式对提供给用户的软件版本进行识别。</p>

<p>版本号用于捕获相关产品修订和变种（通常与可移植性、国际化或者性能特征相关）的信息。目标是采用尽量少的标识符去收集所有信息。遗憾的是，在工业生产中还没有一种标准的版本编号机制，或者说，不存在单一的、通用的算法。更多情况是，需要你针对发布的内容和目标群体做些细微不同的调整。</p>

<p>但不管你的目标群体是什么，完全发布最好通过以下方式进行标识。这是经过不断地反复实践，被各种封闭、开源软件所广泛使用的惯例。</p>

<ul>
<li>软件的名称。</li>
<li>用x.y.z.build四位元组去捕捉修订信息。</li>
<li>根据需要生成的任意变种信息。</li>
</ul>

<p>版本编号中的元组定义：</p>

<table><thead>
<tr>
<th>元组</th>
<th>定义</th>
</tr>
</thead><tbody>
<tr>
<td>x</td>
<td>主版本号。表示产品的当前主要版本。用来表示提供给客户的产品功能的主要增强。在一个极端的例子中，主版本号的增加用来说明产品现在已经拥有一个全新的功能类。</td>
</tr>
<tr>
<td>y</td>
<td>次版本号。表示给产品新增了一些特征，或者是在原来文档中描述的特征上做了重要的修改。用来确定次版本号什么时候需要修改的一个衡量标准就是产品功能说明书。</td>
</tr>
<tr>
<td>z</td>
<td>修订版本号。用来表示给产品所做的缺陷维护行为的等级。产品缺陷是在产品的功能说明书中没有定义，并且已经或者可能对产品的使用者造成不利影响的任何行为。缺陷维护可以看作是支持该版本功能说明的一切活动。</td>
</tr>
<tr>
<td>build</td>
<td>构建版本号。一般是编译器在编译过程中自动生成。</td>
</tr>
</tbody></table>

<p>除z和build的意义比较明确外，对x和y的解释都太笼统。我们需要更加详细的说明以指导我们的开发工作。</p>

<p>修订过的版本编号中的元组定义：</p>

<table><thead>
<tr>
<th>元组</th>
<th>定义</th>
</tr>
</thead><tbody>
<tr>
<td>x</td>
<td>主版本号。用于有扩展性的、客户可见的架构上或特性上的改变。以一个管理大型数据库的系统为例，你可能在以下情况时需要定义一个主要的版本发布：</td>
</tr>
<tr>
<td></td>
<td>* 改变数据库的结构，导致单纯的升级系统对客户产生比较严重的影响。</td>
</tr>
<tr>
<td></td>
<td>* 改变已经发布的API，导致它与前一版本不兼容。</td>
</tr>
<tr>
<td></td>
<td>* 删除功能（好的架构师应该删除一些不需要的功能）。</td>
</tr>
<tr>
<td></td>
<td>* 持续增加新的功能，例如对新的操作系统的支持。</td>
</tr>
<tr>
<td></td>
<td>x的增加也可以是出于纯粹的商业理由。例如，客户的技术支持合同标明，在下个主要版本发布以后，软件可以得到18个月的技术支持。通过增加x，你将强迫客户去升级。</td>
</tr>
<tr>
<td>y</td>
<td>次版本号。通常与期望的功能或其它改进措施相关。当市场部门认为这个版本的一系列特性已经通过证实，次要版本号就会增加。决定增加x或y可能会比较随意。市场架构师应该定义触发任何一个增加的事件（定义与x相关的触发事件比定义y要更容易）。</td>
</tr>
<tr>
<td>z</td>
<td>修订版本号。主要版本号和次要版本号都相同的维护版本应该彼此兼容。</td>
</tr>
<tr>
<td>build</td>
<td>构建版本号。一般是编译器在编译过程中自动生成。</td>
</tr>
</tbody></table>

<p>注意：这里的版本编号规则仅适用于发布周期较长的软件产品。如果发布周期很短，像Chrome和Firefox那样，可能就不太适用。</p>

<h3 id="参考资料">参考资料</h3>

<ul>
<li><a href="http://semver.org/">语义化版本</a></li>
<li>《软件发布方法》</li>
<li>《超越软件架构：创建和维护优秀解决方案》</li>
</ul>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/chasing-the-shiny-and-new-in-software/">追逐时髦的技术</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-08-29T11:50:09+08:00'><span class='date'><span class='date-month'>Aug</span> <span class='date-day'>29</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>11:50 am</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>英文原文：<a href="https://www.nemil.com/musings/shinyandnew.html">https://www.nemil.com/musings/shinyandnew.html</a></p>

<p>有关当前最好的框架或编程语言的争论经常发生在Web开发中。就这点而言，<a href="https://www.scribd.com/">Scribd</a>的联合创始人Jared Friedman在2015年写了<a href="http://blog.jaredfriedman.com/2015/09/15/why-i-wouldnt-use-rails-for-a-new-company/">一篇文章</a>推荐创业公司使用Node.js代替Rails。</p>

<p>他提出几个关键点：</p>

<ul>
<li>Rails很慢。</li>
<li>黑客学院的毕业生都在使用Rails，贬低了它对高级工程师的价值，并减少了它的未来前景。</li>
<li>创业公司应该使用那些前瞻性工程师今后将使用的技术，以保证它们的应用不过时。</li>
<li>在Scribd，过去几年里它们已经从Prototype转换到jQuery，再到CoffeeScript，再到Angular，再到React。</li>
</ul>

<p>Node.js是创业公司的绝佳选择，但它饱受批评的两个部分令我担忧。首先，一名创业公司的工程师应该了解什么技术将会在几年后流行，以保证它们的技术栈不过时。第二，杰出的软件工程师将被时髦的技术栈吸引到创业公司，而不是有趣的技术问题。在过去我还听到过更恶劣的传闻，创业公司的开发者拒绝接受使用ES5 JavaScript编程的工作（那时CoffeeScript刚出来），Mongo发布不久工程师就执意在生产环境下使用Mongo替代Postgres，渴望用最新的前端框架不断重构项目。</p>

<p>我担心有些程序员（和他们的雇主）有这种倾向，即把注意力放在转换技术栈到最新上。他们主要基于框架选择公司，力求在工作中使用最新而不是最好的工具。他们把时间花在新的库和框架上，而不是提高他们的核心技术能力。我们把他们称为技术栈追逐者——他们奋力追求在创业公司的技术栈中使用那些对核心输出（用户重视的软件功能、开发团队的生产力）提升有限的新技术（或者他们自己喜欢的技术）。</p>

<h3 id="“时髦的”web开发">“时髦的”Web开发</h3>

<p>很同情那些在<a href="https://news.ycombinator.com/">Hacker News</a>上的时髦的Web或移动应用开发者。作为在2012年的全栈创业公司的开发者，你正在构建后端使用Ruby/Rails，前端使用Backbone/CoffeeScript/Underscore的网站，同时使用Capistrano（或相关的Python类似物）部署你的应用。到2013年，你已经将后端转换到Node/Express/Mongo，前端为Grunt/Ember。在2014年，你已经彻底切换到MEAN技术栈，但在尝试过Koa以后考虑转移到Go（在Express核心贡献者<a href="https://medium.com/@tjholowaychuk/farewell-node-js-4ba9e7f3e52b">告别Node.js转向Go</a>以后）。在2015年，你在后端使用Express/Go，前端使用Gulp/ES2015/React，使用React Native代替原生移动语言，并且慢慢地将系统转换为使用Docker的微服务。很快，你将会被转换到Phoenix，如果Angular 2是正确的选择也会转换过去——甚至可能创造一个Go可以工作在Android上以及开源Swift可以适合你的技术栈的世界。（我显然是夸大效果，尽管这是HN头条新闻流行什么的一个合理表示。）</p>

<p>有几个原因表明这可能是合理的。时髦的Web工程师需要“时尚”才能获得未来的工作或合同。雇主使用框架或语言作为过滤器，而不是测试批判性思维和技能。雇主没有意识到有实力的开发者如果有正确的支持，可以在几个星期，通常是几天内成为许多语言或者框架的专家。有时趋势是无法阻止的：Swift正在取代Objective-C，世界正在转向更薄、更小的单体后端和更重、反应更灵敏的前端。通常，转变有着巨大的优势：生产力大幅上升，或者新的用户功能突然变得可能。然而，所有的变化都不会导致早期到中期的公司不采用就死，而为了乐趣或业余项目学习技术和认为它是生产环境的关键是迥然不同的。</p>

<p>我们可以用创业公司的时髦的Web或移动开发者与我们的计算机科学家作为对比。我的一个朋友是一家顶级科技公司的计算机神经学家——跟几乎所有从事技术工作的人一样，他的世界每隔几个月就会被重塑——得益于计算能力、脑成像和深度学习算法的快速发展。基本的编程工具其实变化不大。公平地说，只有C++从 11转换到14引起了一些焦虑。还有分布式计算系统、键/值存储和其它外部服务，但这些都是使用稳定的API构建的。他的大部分时间都花在单个DSL中的架构和算法上，而不是重写功能相似的代码或者快速学习提供有争议的好处和改变的库。</p>

<h3 id="选择工具">选择工具</h3>

<p>人们可能会建议创业公司选择时髦的技术栈，因为它是招聘杰出的工程师的关键工具。我自己的观察是，杰出的工程师注重其它的东西。到目前为止，最重要的是提供有趣的问题去解决——有趣的人与他们合作。吸引力和强大的使命感是吸引优秀人才（工程师或者其他）的其它途径。</p>

<p>我并不是在抱怨技术发展太快，也不是说我们都应该用汇编语言或者C++或者Ruby编程。软件工程师清楚他们的目标——我们的领域以令人目眩的速度发展，但对于我们拥有的影响力这都是值得的，因为有10亿人上网。我认为你需要有能力快速地学会新的框架、语言或库（如何完成它的<a href="https://news.ycombinator.com/item?id=7733249">Ask HN</a>）——依靠周围那些经验丰富的工程师，你的目标应该是尽快地具有生产力。除此之外，你应该深刻理解多种语言，而不仅仅是一种（但是同样的态度，不应盲目地扩展到框架或者轻量级的DSL）。</p>

<p>对于创业公司而言，Paul Graham<a href="http://castig.org/an-interview-with-paul-graham-hackers-painters-10-years-later/">在2013年被问到关于理想的语言</a>：“我的意思是，我们有的创业公司在用PHP编写代码——这让我有点担心，但这并不像其它事情那么让我担心。”GitHub的技术主管Sam Lambert在<a href="https://medium.com/s-c-a-l-e/github-scaling-on-ruby-with-a-nomadic-tech-team-4db562b96dcd">最近的一次采访</a>中谈到，他在2013年被GitHub的CTO面试时，对GitHub的技术栈是Rails、C和Bash脚本感到惊讶：“随着面试的继续，我发现他们实际上是一群非常务实的黑客，他们只钻研Ruby和C，使用更稳定的技术栈以便花时间工作在更有趣的事情上，而不是追逐最新最酷炫的技术。”GitHub的方法在我看来是Web和移动开发者的合理的平衡：广泛地探索工具，然后务实地选择解决你所面临的问题的工具（<a href="https://martinfowler.com/bliki/Yagni.html">YAGNI</a>适用于更多的地方，而不仅仅是面向用户的功能开发）。</p>

<p>令我担心的是，某些开发者，特别是在职业生涯早期的开发者，可能会以为创业公司的工程师不是问题解决者或计算机科学家，而是一个荣誉查找表——他们的任务是每隔几个月记住一个新的DSL——只能获得有限的好处。这使我们这些早期的工程师贬值——构建人们想要的东西，从事有趣的技术问题，快速交付代码。</p>

<p>无论如何，要在额外的时间里广泛地实践。如果好处是压倒性的，则切换生产环境中的语言/框架，但要考虑是哪些好处。警惕那些追求新技术却不考虑它对团队的预期优势的人。花时间学习概念和解决有趣的技术或用户问题。如果你有正确的应用边界，并选择你有现成生产力的框架，一旦你这样做了，你将具有一定的灵活性，但需要足够坚持才能达到产品与市场的匹配和超越。</p>

<p>任何一天打开<a href="https://news.ycombinator.com/">Hacker News</a>，你都能看到有帖子诱惑你使用某个框架、语言、类库或者服务去贡献和构建应用（包括一些像Mongo这样有大笔现金的公司，因此在它们的平台后面有营销预算）。有些工具拥有改变游戏规则的能力，其余的只有一些关键的不同功能，但是它们都需要时间才能成为专家。有些工具会大声宣告它们才是未来，并且嘲笑你所学到的东西——但是它们需要你的技能和意识与现有的技术真正地竞争。你会如何选择？</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/what-should-checked-and-unchecked-translate/">checked/unchecked应该翻译成什么？</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-07-29T12:17:06+08:00'><span class='date'><span class='date-month'>Jul</span> <span class='date-day'>29</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>12:17 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>翻译有关Java异常的文章时，总是犹豫是否该把checked/unchecked也翻译过来。原因是，不是很清楚该如何优雅传神地翻译这两个单词。</p>

<p>《Java核心技术》将它们翻译成“已检查/未检查”。《Java编程思想》和《Effictive Java中文版》则翻译成“被检查的/不检查的”。至于技术文章的翻译更是花样百出，有“检测/非检测”、“可检测/非检测”、“可查/不可查”、“受查/非受查”、“检查型/非检查型”、“检查/非检查”等。</p>

<p>到底该翻译成什么呢？在回答这个问题前，让我们先确定什么是checked/unchecked异常？</p>

<p><img src="/uploads/exception-hierarchy.png" alt="exception-hierarchy"></p>

<p>上图是Java中的异常层次结构图。Java语言规范将派生自RuntimeException类和Error类的所有异常称为“unchecked异常”，其它的异常称为“checked异常”。</p>

<blockquote>
<p>The unchecked exception classes are the run-time exception classes and the error classes.</p>

<p>The checked exception classes are all exception classes other than the unchecked exception classes. That is, the checked exception classes are Throwable and all its subclasses other than RuntimeException and its subclasses and Error and its subclasses.</p>
</blockquote>

<p>并且，在编译时编译器会检查程序是否为所有的“checked异常”提供处理器。</p>

<blockquote>
<p>This compile-time checking for the presence of exception handlers is designed to reduce the number of exceptions which are not properly handled.</p>
</blockquote>

<p>从上述的描述可以得出，“checked异常”和“unchecked异常”是两种异常类型，且“checked异常”隐含有必须要检查的思想。</p>

<p>紧紧围绕这些描述，细细地思考和比较，个人认为：1. 《Java核心技术》的翻译存在问题，“已检查”和“未检查”说明的是异常的检查状态，没有表达出异常的分类这个概念。2. 《Java编程思想》和《Effictive Java中文版》的翻译则正确地表达了异常的分类，但“被检查”翻译的有点无厘头，如果能改成“要检查”则会更好，缺陷是连接“异常”这个词组后是短语，而非名词，读来费劲，也不上口；如果去掉“的”的话，后者会有歧义，听起来像是命令。3. “检测/非检测”和“检查/非检查”是同个意思。4. “可检测”这个翻译看上去似乎表示异常是可以检查的，和Java语言规范要求的该类异常必须要检查不符。5. “可查/不可查”也是如此。6. “受查/非受查”的翻译则有些莫名其妙的感觉。7. “检查型/非检查型”翻译的很好，既表达了异常的分类，也表达了一种异常是要检查的，另一种异常是不要检查的意义，只是前者还缺少点强制的意味。</p>

<p>分析到这里，结果已经是不言而明。“要检查的/不检查的”和“检查型/非检查型”是两种更好的翻译，都能把Java语言规范对checked/unchecked异常的描述尽量地表述出来。而后者在实际使用中更为简洁适宜。</p>

<p>接下来的事情就是把以前译文中未翻译的checked/unchecked修改成“检查型/非检查型”。在以后的翻译中也继续使用这个翻译结果，除非能找到更好的表述方式。</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/2">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/how-to-verify-mailbox-exist/">如何验证邮箱是否存在？</a>
      </li>
    
      <li class="post">
        <a href="/blog/crawl-novel-with-scrapy-part6/">使用Scrapy爬取小说（6）</a>
      </li>
    
      <li class="post">
        <a href="/blog/crawl-novel-with-scrapy-part5/">使用Scrapy爬取小说（5）</a>
      </li>
    
      <li class="post">
        <a href="/blog/crawl-novel-with-scrapy-part4/">使用Scrapy爬取小说（4）</a>
      </li>
    
      <li class="post">
        <a href="/blog/crawl-novel-with-scrapy-part3/">使用Scrapy爬取小说（3）</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Recent Comments</h1>
  <div class="dsq-widget">
    <script type="text/javascript" src="http://codemany.disqus.com/recent_comments_widget.js"></script>
  </div>
</section>
<section>
  <h1>Tags</h1>
  <ul class="tag-cloud">
    <a style="font-size: 140%" href="/tags/antlr/">ANTLR</a>
<a style="font-size: 106%" href="/tags/algorithm/">Algorithm</a>
<a style="font-size: 133%" href="/tags/android/">Android</a>
<a style="font-size: 96%" href="/tags/c-plus-plus/">C++</a>
<a style="font-size: 106%" href="/tags/css/">CSS</a>
<a style="font-size: 91%" href="/tags/crack/">Crack</a>
<a style="font-size: 96%" href="/tags/crawler/">Crawler</a>
<a style="font-size: 96%" href="/tags/database/">Database</a>
<a style="font-size: 114%" href="/tags/git/">Git</a>
<a style="font-size: 96%" href="/tags/html/">HTML</a>
<a style="font-size: 103%" href="/tags/jbookshelf/">JBookShelf</a>
<a style="font-size: 137%" href="/tags/java/">Java</a>
<a style="font-size: 100%" href="/tags/javascript/">JavaScript</a>
<a style="font-size: 96%" href="/tags/listview/">ListView</a>
<a style="font-size: 96%" href="/tags/mysql/">MySQL</a>
<a style="font-size: 103%" href="/tags/paperclip/">Paperclip</a>
<a style="font-size: 96%" href="/tags/python/">Python</a>
<a style="font-size: 116%" href="/tags/qianbao/">Qianbao</a>
<a style="font-size: 150%" href="/tags/rails/">Rails</a>
<a style="font-size: 117%" href="/tags/ruby/">Ruby</a>
<a style="font-size: 96%" href="/tags/scrapy/">Scrapy</a>
<a style="font-size: 91%" href="/tags/spring/">Spring</a>
<a style="font-size: 100%" href="/tags/struts/">Struts</a>
<a style="font-size: 145%" href="/tags/translation/">Translation</a>
<a style="font-size: 96%" href="/tags/ubuntu/">Ubuntu</a>
<a style="font-size: 106%" href="/tags/windows-server-2008/">Windows Server 2008</a>
<a style="font-size: 106%" href="/tags/wxwidgets/">wxWidgets</a>

  </ul>
</section>
<section>
  <h1>Blogroll</h1>
  <ul>
    <li><a href="http://mindhacks.cn/">刘未鹏MIND HACKS</a></li>
    <li><a href="http://www.importnew.com/">ImportNew</a></li>
    <li><a href="http://ifeve.com/">并发编程网</a></li>
    <li><a href="http://codingnow.com/">云风的 BLOG</a></li>
    <li><a href="http://dbanotes.net/">DBA Notes</a></li>
    <li><a href="http://www.ruanyifeng.com/blog/">阮一峰的网络日志</a></li>
    <li><a href="http://advdbg.org/">高端调试</a></li>
    <li><a href="http://www.cs.usfca.edu/~galles/visualization/Algorithms.html">Data Structure Visualization</a></li>
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <a href="https://github.com/dohkoos">@dohkoos</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'dohkoos',
            count: 5,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>

<section>
  <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a>
  <div>This work is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.</div>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - dohkoos -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'codemany';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>





</body>
</html>
